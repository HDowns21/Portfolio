# -*- coding: utf-8 -*-
"""downs_harrison_a2.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1CYcpnTNp-Z6E1hJXZuVxsiSHzm94sWQe
"""

#Harrison Downs
#B01256060
#09/06/2024
#CSCI 6385 Assignment 2

import heapq
import random

#goal_positions serves as a lookup for any given tile's goal position in terms
#of 2D coordinates
goal_positions = {
    1: (0, 0), 2: (0, 1), 3: (0, 2),
    4: (1, 0), 5: (1, 1), 6: (1, 2),
    7: (2, 0), 8: (2, 1), 0: (2, 2)
}

#move_positions serves as a lookup for what number to add to blank_i when
#performing an action

move_positions = {
    'u': -3, 'r': 1, 'd': 3, 'l': -1
}

#tracks each type of search algorithm being used; once a search algorithm is
#used, the starting state will make a note of it.
search_algorithms = {
    1: "A*",
    2: "Greedy"
}

class S:

  """

  The S class holds crucial data and methods which will be utilized by a
  search algorithm.

  DATA:
  - state: stores current state of the board in a 1D array
  - parent: stores the first direct ancestor of the current state
  - move: stores the action that occurred to create the current state
  - depth: tracks the # of actions that have occurred from the starting state
  - blank_i: stores the position of the blank tile in the current state
  - search_alg: stores a number representing which search algorithm was used
    to find the solution

  METHODS:
  - __lt__(): required for heapq to compare states based on their g+h value
  - h2(): calculates distance of every tile to the goal state (heuristic)
  - expand(): ascertains which actions are possible from the current state;
    returns list of potential states resulting from those actions
  - create_state(): takes in a state and action, and returns the state that
    results from that action
  - get_solution_path(): returns a list of actions that leads from the start
    state to the goal state

  """

  def __init__(self, state, parent = None, move = None, depth = 0, blank_i =
               0, search_alg = None):

    self.state = state
    self.parent = parent
    self.move = move
    self.depth = depth
    self.blank_i = self.state.index(0)
    self.search_alg = search_alg

    #TESTING
    #print_S(self)

  def __lt__(self, other):

    #A* search considers backward and forward cost
    if self.search_alg == 1:

      #TESTING
      #print("Current depth: ", self.depth)
      #print ("Current forward cost: ", self.h2())
      #print("Other depth: ", other.depth)
      #print("Other forward cost: ", other.h2())

      return (self.depth + self.h2()) < (other.depth + other.h2())

    #greedy search only considers forward cost
    elif self.search_alg == 2:
      return self.h2() < other.h2()

  def h2(self):

    #distance will store the heuristic value to be returned
    distance = 0

    #get each tile and index from the state; only do so for non-blank tiles
    for index, tile in enumerate(self.state):
      if tile != 0:

        #translate each tile's index into 2D coordinates
        curr_x, curr_y = divmod(index, 3)

        #obtain each tile's goal coordinates
        goal_x, goal_y = goal_positions[tile]

        #keep running total of each tile's manhattan distance to goal pos
        distance += abs(goal_x - curr_x) + abs(goal_y - curr_y)

    return distance

  def expand(self):

    #list of each possible state that may be reached from the current state
    neighbors = []
    #translate blank space's index into 2D coordinates
    curr_x, curr_y = divmod(self.blank_i, 3)

    #check if blank space is on an edge; only accept possible actions. Then,
    #append the state resulting from each valid action to neighbors
    if curr_x != 0:
      neighbors.append(self.create_state('u'))

    if curr_y != 2:
      neighbors.append(self.create_state('r'))

    if curr_x != 2:
      neighbors.append(self.create_state('d'))

    if curr_y != 0:
      neighbors.append(self.create_state('l'))

    #TESTING
    #print("Neighbors:")
    #for neighbor in neighbors:
      #print_S(neighbor)

    return neighbors

  def create_state(self, action):

    #copy a mutable version of the current state
    new_state = list(self.state)

    #reference move_positions lookup to identify the index the blank space is
    #moving into
    target_i = self.blank_i + move_positions[action]

    """
    print(self.state)
    print(self.blank_i)
    print(action)
    print(target_i)
    """

    #exchange tiles between the two positions
    new_state[self.blank_i] = new_state[target_i]
    new_state[target_i] = 0

    #create a new S object to return with the new state
    new_S = S(tuple(new_state), self, action, self.depth + 1, target_i,
              self.search_alg)

    #TESTING
    #print("New state:")
    #print_S(new_S)

    return new_S

  def get_solution_path(self):

    #empty list to hold moves along solution path
    path = []
    curr_S = self

    #iterate through each parent state until start state is reached, adding
    #moves to path list
    while curr_S.parent is not None:
      path.append(curr_S.move)
      curr_S = curr_S.parent

    #reverse the path list to show steps from the start state to the goal state,
    #then return that list as the solution path
    return path[::-1]

class Solution_Table:

  """

  This class is used to record the statistics of multiple executions of each
  search algorithm.

  DATA:
  - d: stores estimated solution depth. Used by random_state() to determine how
    many times to perform a random tile shift before returning the randomized
    state.
  - num_times_to_run: stores the number of times to run the search algorithm.
    For each run, the total cost and solution depth will be recorded, and at the
    end, this class can calculate the average total cost and solution depth.
  - starting_states: holds each randomized starting state. Parallel to
    total_cost and solution_depth.
  - total_cost: holds the total cost of each run. Parallel to starting_states
    and solution_depth (unique one for each search alg).
  - solution_depth: holds the solution depth of each run. Parallel to
    starting_states and total_cost (unique one for each search alg).

  METHODS:
  - __init__: takes search_alg, n, and num_times_to_run as arguments.
    Immediately populates starting_states, total_cost, and solution_depth by
    running the given search algorithm num_times_to_run times.
  - get_avg_total_cost(): returns the average total cost of all runs.
  - get_avg_solution_depth(): returns the average solution depth of all runs.
  - print: prints the average total cost and solution depth of all runs.

  """

  def __init__(self, d, num_times_to_run):

    self.d = d
    self.num_times_to_run = num_times_to_run
    self.starting_states = []

    self.a_star_total_cost = []
    self.a_star_solution_depth = []

    self.greedy_total_cost = []
    self.greedy_solution_depth = []

    goal_s_obj = S((1, 2, 3, 4, 5, 6, 7, 8, 0))
    cost_count = 0

    #generate random starting states
    for i in range(num_times_to_run):
      self.starting_states.append(random_state(d))

    #run search alg for each state, recording total_cost and solution_depth
    for starting_s_obj in self.starting_states:
      goal_s_obj, cost_count = a_star_search(starting_s_obj)

      #once each search is completed, record its total cost and solution depth
      self.a_star_total_cost.append(cost_count)
      self.a_star_solution_depth.append(len(goal_s_obj.get_solution_path()))

    for starting_s_obj in self.starting_states:
      goal_s_obj, cost_count = greedy_search(starting_s_obj)

      self.greedy_total_cost.append(cost_count)
      self.greedy_solution_depth.append(len(goal_s_obj.get_solution_path()))

  def get_avg_total_cost(self, alg):

    if alg == 1:
      return sum(self.a_star_total_cost) / len(self.a_star_total_cost)

    elif alg == 2:
      return sum(self.greedy_total_cost) / len(self.greedy_total_cost)

  def get_avg_solution_depth(self, alg):

    if alg == 1:
      return sum(self.a_star_solution_depth) / len(self.a_star_solution_depth)

    elif alg == 2:
      return sum(self.greedy_solution_depth) / len(self.greedy_solution_depth)

  def print(self):

    #used for f string
    a_star = "A Star"
    greedy = "Greedy"

    #print in table format
    print("d = ", self.d, ": ")
    print(f"{a_star:<10} {self.get_avg_total_cost(1):<10}"
          f"{self.get_avg_solution_depth(1):<10}")
    print(f"{greedy:<10} {self.get_avg_total_cost(2):<10}"
          f"{self.get_avg_solution_depth(2):<10}")
    print("\n")

def is_goal_state(s):

  """

  This function returns true if a given state is equivalent to the goal state,
  and false if not

  """

  if s == (1, 2, 3, 4, 5, 6, 7, 8, 0):
    return True
  else:
    return False

def print_S(S):
  print(S.state[0:3])
  print(S.state[3:6])
  print(S.state[6:9])
  print("Previous move: ", S.move)
  print("Depth: ", S.depth)
  print("h(n): ", S.h2())

def random_state(d):

  """

  This function generates a random, solvable state of the board. No states are
  visited more than once.

  """
  #start with goal state
  starting_s_obj = S((1, 2, 3, 4, 5, 6, 7, 8, 0))


  visited = set()                    #remembers each previously visited state
  state_stack = [starting_s_obj]     #tracks current path; if a state has no
                                     #valid moves, this is used to backtrack.
  depth_counter = 0     #ensures exactly d moves are made
  visited.add(starting_s_obj.state)

  #assign this before loop to prevent errors
  current_s_obj = starting_s_obj

  while depth_counter < d:

    current_s_obj = state_stack[-1]
    neighbors = current_s_obj.expand()
    valid_neighbors = []

    #filter out states that have already been visited
    for n in neighbors:
      if n.state not in visited:
        valid_neighbors.append(n)

    if valid_neighbors:
      #choose a random, unvisited neighbor
      next_s_obj = random.choice(valid_neighbors)
      state_stack.append(next_s_obj)
      visited.add(next_s_obj.state)
      depth_counter += 1

    else:
      #backtrack if no valid moves are available
      state_stack.pop()

  final_s_obj = state_stack[-1]

  #reset attributes to function as the starting state in a search
  final_s_obj.parent = None
  final_s_obj.move = None
  final_s_obj.depth = 0
  final_s_obj.blank_i = final_s_obj.state.index(0)

  return final_s_obj

  """
  for i in range(n):

    #pick a random action to take
    return_s_obj = random.choice(return_s_obj.expand())

  #reset parent, depth, move, blank_i
  return_s_obj.parent = None
  return_s_obj.move = None
  return_s_obj.depth = 0
  return_s_obj.blank_i = return_s_obj.state.index(0)

  return return_s_obj
  """
def a_star_search(start_s_obj):

  """

  This function takes in an S object representing a start state, and returns a
  goal state (or None if no solution is found). The A* search algorithm is used,
  which takes into account each state's backward cost and estimated forward
  cost.

  """
  cost_count = 0
  start_s_obj.search_alg = 1
  frontier = []           #priority queue for unexpanded nodes
  visited = set()         #remembers each previously visited state

  heapq.heappush(frontier, start_s_obj)

  while frontier:

    #TESTING
    #print("********************************")
    #for obj in frontier:
      #print(obj.depth + obj.h2(), obj.depth, obj.h2())

    curr_S = heapq.heappop(frontier)

    #print("Current state:")
    #print_S(curr_S)

    #print("Current depth: ", curr_S.depth)
    #print("Current forward cost: ", curr_S.h2())

    if is_goal_state(curr_S.state):
      return curr_S, cost_count

    else:
      cost_count += 1
      visited.add(curr_S.state)

      for S in curr_S.expand():
        if S.state not in visited:
          heapq.heappush(frontier, S)

  return None, cost_count #if no solution is found

def greedy_search(start_s_obj):

  """

  This function takes in an S object representing a start state, and returns a
  goal state (or None if no solution is found). The greedy search algorithm is
  used, which only takes into account each state's estimated forward cost.

  """
  cost_count = 0
  start_s_obj.search_alg = 2
  frontier = []           #priority queue for unexpanded nodes
  visited = set()         #remembers each previously visited state

  heapq.heappush(frontier, start_s_obj)

  while frontier:

    #TESTING
    #for obj in frontier:
      #print_S(obj)

    curr_S = heapq.heappop(frontier)

    #print("Current state:")
    #print_S(curr_S)

    if is_goal_state(curr_S.state):
      return curr_S, cost_count

    else:
      cost_count += 1
      visited.add(curr_S.state)

      for S in curr_S.expand():
        if S.state not in visited:
          heapq.heappush(frontier, S)

  return None, cost_count #if no solution is found

#Print the average total cost and solution depth for A Star & Greedy Search in
#table format:

print("           Cost      Depth")

for i in range(32):

  table = Solution_Table(i, 1000)
  table.print()